In this chapter we define problems we will be focusing on, and give elementary goals for the analysis tool -- what it needs to be able to handle to be considered useful.
We will also present concepts which we will use later in the work and introduce programming languages used in the examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program Analysis} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, we will not be concerned with simple program analysis techniques like type-checkers, linters.
Instead, we will focus on techniques based on formal verification, that is techniques which can provide some formally described guarantees about the program if the succeed.
These techniques usually require a program and some specification which the program should adhere to, and can check if that is really the case.
We will also focus mostly on techniques which can be directly applied to programs written in some mainstream programming language (i.e., techniques which do not require special verification-oriented languages as their inputs).

Program analysis techniques can be broadly divided into two different areas, \emph{automatic techniques} which, provided a program and its specification, should produce a result automatically without further assistance, and \emph{human-asisted techniques} which require a substantial human effort during the analysis.
Examples of the first area are symbolic analysis and various model checking techniques, while theorem proves are example of the latter kind.
As we focus on techniques which should be usable by programmers without a substantial background in logic and programming, we will focus primarily on the automatic tools.


\subsection{State Space}

In order to be able to define program properties which should be checked and to describe various analysis methods, we first need to define the \emph{state space of the program}.

\TODO{finite-state threads}

\begin{definition}[State Space]
The \emph{state space} of a program is a directed multigraph with (optionally) labelled edges which describes all the ways in which the program can be executed.
The vertices of the state space multigraph are called \emph{states} (of the program).
Each state represents a certain point in the execution of the program -- it can be described by a snapshot of the program (its memory, program counters and stacks of all its threads, …).
States $v_1, v_2$ are connected by an edge in the state space if $v_2$ can be reached from $v_1$ by a single step of the program.
The edge can be optionally labelled, for example by the statements executed in each step, or by a selected actions specified by the program.
The state space contains all the states of the program which are reachable from a certain initial state, i.e., an initial configuration of the program.
\end{definition}

The state space is usually given not as a graph, instead it is specified by its
implicit representation -- the program code, or possibly a function which
describes the initial states and how to get from one state to its successors.
In practice, the state space of a program can be very large or infinite and it
is useful to be able to consider only a representative part of the state space.

\begin{definition}[Reduced State Space]
A \emph{reduced state space} of a program is a subset of the state space (\TODO{grafový pojem?}) in which states are subset of the original state space and edges connect states between which there is a directed path in the original state space such that all the internal states of this path are not contained in the reduced state space.
\end{definition}

There are several techniques which can be used to construct reduced state space in such a way that a given property (or a class of properties) holds in the reduced state space if and only if it holds in the original state space.

Finally, we will need the following state-space-related definitions.

\begin{definition}[Finite State Space]
We say that a program has a \emph{finite state space} if the number of vertices in the state space is multigraph is finite.
Otherwise, we say the program has an \emph{infinite state space}.
\end{definition}

\begin{definition}[Run]
A \emph{run} in the state space is a (possibly infinite) path in the state space (or reduced state space) which starts in the initial state.
That is, a run is a sequence of states $\sigma = s_0, s_1, …$ such that $s_0$ is the initial state and for each consecutive pair of states $s_i, s_{i+1}$ there is an edge from $s_i$ to $s_{i + 1}$ in the corresponding (reduced) state space.
Please note that states can be repeated in the run.
\end{definition}


\subsection{Program Properties}

In order to be able to find errors in programs an analysis tool has to have
some specification of program correctness -- a \emph{property} which should be
checked.
In this section, we will introduce several categories of program properties
together with examples of particular properties.

\begin{definition}[Safety Properties]
\emph{Safety properties} are properties which can be checked locally -- for
each state of the program, we are able to determine if the state satisfies or
violates given property.
\end{definition}

Therefore, to conclude the program is error-free under given safety property it
suffices to show that there is no state which violates given safety property.

Examples of safety property include \emph{memory safety} (stating that every
time we access a certain part of memory, this memory is in fact allocated and
accessible to the program, see also \autoref{fig:pre:memsafe}), \emph{assertion
safety} (stating that a program does not violate any of the assertions in the
source code, see also \autoref{fig:pre:assert}), and \emph{control-flow
definedness} (stating that every time a conditional jump is performed, the
values the jump is based on must have well-defined value, see also
\autoref{fig:pre:definedness}).
Some program properties can be checked locally only if sufficient information
is kept in the program state.
For example checking \emph{absence of memory leaks} requires that it is
possible to enumerate all allocated objects and all objects to which there are
usable pointers.

\bigskip

\TODO{Temporal properties

Not all properties can be directly described as safety properties.
For example, we can have a property stating ,,every time a button is pressed, the elevator will eventually drive to the floor the button was pressed on'' -- such a property cannot be checked directly in one state of the program, but instead needs to be checked on a \emph{run} of the program.

For specification of temporal properties, we will use temporal logic CTL*~\cite{todo}, and its linear fragment, LTL (Linear Time Logic)~\cite{TODO}.

\begin{definition}[CTL*]

\end{definition}

\begin{definition}[LTL]
\end{definition}

- more expressive temporal logics…

\subsubsection{Fairness}

\subsubsection{Termination Properties}

A distinguished form of temporal properties are termination properties, which are properties which allow us to specify that a program must (or must not) terminate, or that some of its part must (or must not) terminate.
An example of a termination property concerning a part of the program can be found in \autoref{fig:pre:term}.

\subsubsection{Other Properties}
}



\subsection{Analysis Techniques}

\subsubsection{Symbolic Execution}

\subsubsection{Model Checking}

\paragraph{Explicit-State Model Checking}

\paragraph{Stateless Model Checking}

\paragraph{SAT/SMT-Based Model Checking}

- bounded, IC3, K-induction


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Realistic Programs} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order for a program analysis tool to be useful to programmers, it should be able to process the code they are creating with a minimal extra effort.
For this reason, program analysis tools should be able to process some of the mainstream high-level programming languages preferably without limitations to the features of these languages which can be understood by the program analyser.
For example a tools which understands basics of the C language, but does not allow dynamic memory allocation, is not very useful for a programmer, as they will probably not be able to  use it to analyse their usual programs which contain dynamic memory allocation.

On the other hand, if the language support is good enough, it enables the programmer to use the analysis tool directly on their program or its fragments, both during the development, and for older code as the programmer is not forced to simplify, rewrite or model their code in order to get usable analysis results.

\subsection{Minimal Requirements for Realistic Programs}

To make our considerations mostly independent of the choice of the verified
formalism (programming language), we will now define minimal requirements for
such a formalism in order to be considered realistic.

\begin{description}
    \item[Function Calls and Recursion] It must be possible to structure the
        program into functions, with possibility to define recursive functions.

    \item[Dynamic Memory Allocation] It must be possible to allocate (and
        deallocate) memory at the runtime of the program, with amount of the
        allocated memory determined at runtime.

    \item[Arrays and indexing] The program must be able to use arrays and index
        them by runtime values.

    \item[Threads] The verified formalism should have ability to spawn threads
        dynamically and to wait for end of a thread.
        It must be possible for any thread to spawn additional threads (i.e.,
        it is not sufficient to have all threads spawn by the first thread).
        Therefore, the number of threads which will run at any point in the
        program might not be known before the program is executed.

    \item[Shared Memory] There should be memory locations accessible by any
        thread.
        This memory can be used to share data between threads.

    \item[Synchronization Primitives] Primitives which allow safe
        synchronization of data between threads (such as mutexes and condition
        variables) should be available.

    \item[Atomic Operations] It has to be possible to read and modify
        designated memory locations with atomic operations which are safe to be
        executed in parallel (e.g., atomic increment, atomic compare and swap).

    \item[Indirect Function Calls] It must be possible to pass functions (or
        pointers to functions) as arguments to functions and to store them into
        data types.
        It must be possible to later call these functions (indirect function
        calls).
        Indirect function calls are used for example to implement dynamic
        dispatch in object oriented programming, when staring threads, and for
        higher-order functions (functions which take other functions as
        arguments, e.g., a \texttt{sort} function can take a function used to
        compare elements of the given array.

    \item[Assertions] It must be possible to express assertions (i.e.,
        conditions which must hold at given point in the program) in the
        analysed formalism.
        Assertions are used as a basic form of correctness specification.

    \item[Pointers and Pointer Arithmetic] Since we are concerned with C and
        C++, we will consider pointers to be a basic feature (if we were to
        primarily focus to languages with no direct access to pointers (such as
        Java, C\# or Python), this would not be the case).

        With pointers and pointer arithmetic, it is possible to have pointers
        pointing inside of objects (not at their beginning), or even pointers
        whose type does not match the type of the object they point to (e.g.,
        a~pointer to an integer might be converted to a pointer to a
        floating-point number and then dereferenced).
\end{description}

\subsection{Additional Requirements for Programs}

Sometimes we will also consider additional features which many programming
languages share, but which might not be considered to be critical in a
programming language used for concurrent software.

\begin{description}
    \item[Exception Handling] Exceptions are widely used form of error handling.
        They allow the error to be reported at the point where it is
        encountered, but to be handled by some of the callers of the current
        function.
        This mitigates the problem of manual propagation of error codes from
        functions and therefore might lead to cleaner code.
        Exceptions are often used in higher-level programming languages.
        Sadly, exceptions introduce another way in which functions can end, and
        therefore can be hard to include in a program analysis tool.

    \item[Run-Time Type Support \& Dynamic Dispatch] In object-oriented
        languages (and in dynamically typed languages), it is sometimes
        necessary to be able to determine type of a given object at runtime,
        and to execute different implementations of object's member functions
        based on its type.
        Depending on the way in which the program analysis is implemented, the
        analysis tool might need to be able to understand type informations
        and relation between types.
\end{description}

\subsection{Libraries \& Environment}

Apart from the programming language features mentioned in the previous section,
programs usually do not build everything from scratch but use various libraries
instead.
Usually, basic functionality is provided by the library shipped with the
compiler (\emph{standard library} in case of C and C++), and more advanced
features are provided by various third-party libraries or libraries which
developed alongside the program in question.

Therefore a reasonable program analysis tool must have support for (i) the
languages' standard library and (ii) ability to handle programs which use other
libraries.

In either case, it is possible to either provide a verification-specific
abstraction of the given library, or to analyse the library directly (re-use
the library in the program-analysis environment).
In practice, in order to reuse a library for program analysis, the program
analysis tool has to be able to understand all the features used in
implementation of the given library.
Furthermore, low-level libraries (such as the C standard library) rely on
operating system APIs to provide some of its functionality -- for example, when
a program requires memory to be allocated using the \texttt{malloc} function,
the standard C library will request the memory to be allocated by the operating
system.
This means that if a library which relies on these APIs is reused, either the
verification tool must closely replicate API of some of the operating system
the library can work on, or the library has to be ported for the given
verification tool.

\paragraph{Environment and Operating System} As we have already mentioned in
the case of memory allocation, programs often do not run in isolation.
Instead they can interact with other programs and the operating system using
the filesystem, network, or various inter-process communication protocols.
In order to analyse programs which do such interactions, a verification tool
has to be able to understand these interactions.

\subsection{Hardware-Related Considerations}

When a program is executed on a given hardware, some of its behaviour might
depend on the concrete features of the hardware.
Therefore, in order for the program analysis to be usable, one must know what
hardware model(s) the program analysis tool adheres to and what is their
relation to the real hardware the program will be executed on.
In pracitce, it is often hard to prove that a program will be correct on
\emph{any} hardware on which it will be possible to compile it.

For example, the size (and therefore precision) of the standard C/C++ type
\cpp{int} might depend on the hardware the program is using: on an 8-bit or
16-bit embedded microprocessor, \cpp{int} will likely be a 16-bit type with a
maximum value of $2^{15} - 1$, while on a common 64-bit (or 32-bit) computer it
will be a 32-bit number with maximum value of $2^{31}-1$.
This means a code which causes a bug due to integral overflow on a 16-bit
embedded microprocessor might be correct on a 64 -bit machine.

Another example concerns alignment of values in the memory -- often it is
possible to address single bytes in the memory, but certain data types (which
are larger that a byte) are only allowed to exist on adresses divisible by a
given \emph{alignment}.
For example, an alignment of a 32-bit \cpp{int} type might be 4 bytes (32
-bits).
Depending on the hardware platform, reading and writting unaligned values might
work (e.g., on x86 and x86-64), might trigger an error (e.g., on certain ARM
processors \cite{??}) or the address might be silently rounded to the nearest
aligned address (e.g., on some embedded ARM processors \cite{??}).
Therefore, behaviour of the same program with an unaligned read can differ
drastically depending on the platform on which it is executed.

Similarly, the execution of a parallel program depends on the memory model of
the used processor.

\subsection{Level of verification - TODO (C++ x LLVM x ASM)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallelism \& Threading Model} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Parallelism is an important part in programming high-performance software which
can fully take advantage of the current hardware.
However, parallelism comes with additional problems not present in development
of sequential software, namely it is significantly harder to create correct
parallel software compare to development of sequential software, and it is also
hard to create parallel software which scales well with the number of processors
(or processor cores) available.
Furthermore, improving scalability usually makes the problem of correctness even
harder -- improving scalability usually requires more more fine-grained (and
therefore error-prone) synchronization.
For example, fine-grained locking or lock-free programming can lead to
high-performance code, but seeing that the code uses synchronization properly
is significantly harder then for code which uses a few locks for which it is
well defined which parts of the shared memory they guard.

Furthermore, relaxed memory behaviour comes into play once shared variables are
not only accessed in critical sections.
As processor manufactures strive to increase speed of processors, they have
introduced various optimizations into the memory infrastructure to avoid
waiting for the relatively slow main memory.
These optimizations can (and in most processors are) be visible to a parallel
program and are manifested in relaxed memory behaviour.
For example, on x86 processors (which are used in most laptop, desktop and
server computers) a memory store can be delayed and appear, from the point of
view of the other threads, later than a subsequent load.
Some processors (for example POWER and higher-performance ARM processors) allow
even more reordering, for example two memory stores executed sequentially by
some thread can appear to be preformed in different order by two independent
observer threads.

In this section, we describe the basic model of parallelism we assume for our
programs and follow some of its implications.
We also outline describe basics of relaxed memory behaviour and memory models
which describe it.

\subsection{Basic Threading Model \& Memory Models}

We assume that a program consists of threads which interact using a shared
memory.
We also assume that execution of a thread can be interrupted at any point by a
\emph{thread scheduler} (of the operating system on which the program runs) and
will be later eventually resumed (unless the program exits in the meantime or
the thread is \emph{blocked} infinitely).
A thread can be \emph{blocked} if it waits for a resource provided by the
operating system, for example availability of a lock or input from an input
device.
If a thread cannot be resumed because it is blocked and does never cease to be
blocked we say there is a \emph{deadlock}.

\begin{definition}[Dealock, Partial Dealock]\label{def:deadlock}
    A \emph{deadlock} happens if a scheduler never allows the thread to run
    because it waits for a resource which never becomes available.
    Sometimes, when we need to distinguish between all the threads being
    blocked and only some of them being blocked, we will use the notion of
    \emph{partial deadlock} to signify a situation in a program where some
    threads are blocked (in deadlock) but other can still proceed.
\end{definition}

It is important to note that our definition of deadlock does not include
\emph{busy waiting} -- i.e., a thread is executing a loop in which it tests
that some event occurred.
Busy waiting can be used for example to implement exclusive sections without
operating system support.

\begin{definition}[Livelock]\label{def:livelock}
    \TODO{
    A \emph{livelock} is a situation in which a thread is allowed to run (by
    the scheduler), but does not proceed in any meaningful way because it
    executes a loop which is waiting for an event which ever happens.
    }
\end{definition}

\paragraph{Interleaving of Threads, Sequential Consistency}

In practice program threads can run concurrently, i.e., multiple cores of the
processor can execute different threads at the same instant in time.
Furthermore, the scheduler can map threads to processor cores arbitrarily and
it can even change the core which executes a given thread.
It is not practical (and not necessary) to simulate this behaviour when
performing analysis of a parallel program.
Instead, we consider that threads are interleaved (\emph{interleaving
semantics}) or we consider execution based on some relaxed memory model.

\begin{definition}[Interleaving Semantics of Threads]
    \TODO{
    With the \emph{interleaving semantics} we assume that all possible
    executions of parallel program can be obtained by interleaving.
    That is, at any point in execution of the program we consider which threads
    are allowed to run and explore all possible selections, after a thread
    performs an action, we again consider all possible actions of all threads
    and so on.
    …
    }
\end{definition}

When considering relaxed memory models, it is also useful to consider memory
model which corresponds to the interleaving semantics of threads.
This is the \emph{sequential consistency}.

\begin{definition}[Sequnetial Consistency]
    \emph{Sequential consistency} is the memory model which corresponds to the
    interleaving semantics of threads.
    Under sequential consistency, all effects of memory manipulating
    instructions are visible to all threads immediately and no instruction
    reordering is observable.
\end{definition}

Sequential consistency is the strongest memory model (it is not relaxed at all).
Therefore, a program running under a relaxed memory model will always exhibit
behaviour present if the same program was run under sequential consistency, but
it can also exhibit other behaviour.

\paragraph{Relaxed Memory Models}

Interleaving is not a sufficient abstraction in presence of relaxed memory
behaviour, which is manifested by modern processors.
The relaxed behavior of processors arises from optimizations in cache
consistency protocols and observable effects of instructions reordering and
speculation.
The effect of this behavior is that memory-manipulating instructions can appear
to be executed in a different order than the order in which they appear in the
thread, and their effect can even appear to be in different order on different
threads.
For efficiency reasons, virtually all modern processors (except for very simple
ones in embedded microcontroller) exhibit relaxed behavior.
The extent of this relaxation is dependent on the processor architecture (e.g.,
x86, ARM, POWER) but also on the concrete processor model.
Furthermore, the actual behavior of the processor is often not precisely
described by the processor vendor~\cite{x86tso}.
To abstract from the details of particular processor models, \emph{relaxed
memory models} are used to describe (often formally) behavior of a given
processor architecture.
Examples of relaxed memory models of modern processors are the memory model of
x86 and x86\_64 CPUs described formally as \xtso~\cite{x86tso} and the multiple
variants of POWER~\cite{Sarkar2011,Mador-Haim2012} and
ARM~\cite{Flur2016,Alglave2014,Pulte2017} memory models.

For the description of a memory model, it is sufficient to consider operations
which affect the memory.
These operations include loads (reading of data from the memory to a register
in the processor), stores (writing of data from a register to the memory),
memory barriers (which constrain memory relaxation), and \emph{atomic compound
operations} (read-modify-write operations and compare-and-swap operation).

\subsection{The \xtso Memory Model}

\TODO{Převzato z …}

The \xtso memory model is a formal description of the memory model used in x86
and x86-64 processors (manufactured by both Intel and AMD).
It is one of the strongest relaxed memory models -- it is relaxed compared to
sequential consistency, but not nearly as relaxed as some of the other common
memory models such as memory models of ARM and POWER processors.
The \xtso is very similar to the SPARC Total Store Order (TSO) memory
model~\cite{SPARC94}.
It does not reorder stores with each other, and it also does not reorder loads
with other loads.
The only relaxation allowed by \xtso is that a store can appear to be executed
later than a load which succeeds it.
The memory model does not give any limit on how long a store can be delayed.
An example of non-intuitive execution of a simple program under \xtso can be
found in Figure~\ref{fig:xtso}.

\begin{figure}[th] % fig:xtso
    \begin{minipage}[b]{0.25\textwidth}
    \tt
    \textcolor{gray}{int} x = 0, y = 0;
    \par\smallskip
    \textcolor{gray}{void} thread0() \{ \\
    \indent{}y = 1; \\
    \indent{}\textcolor{gray}{int} a = x; \\
    \indent{}\textcolor{gray}{int} c = y; \\
    \}
    \par\smallskip
    \textcolor{gray}{void} thread1() \{ \\
    \indent{}x = 1; \\
    \indent{}\textcolor{gray}{int} b = y; \\
    \indent{}\textcolor{gray}{int} d = x; \\
    \}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}[b]{0.73\textwidth}
    \begin{center}
    \noindent
    Is $a = 0 \land b = 0$ reachable?\\[2.5ex]
    \begin{tikzpicture}[ ->, >=stealth', shorten >=1pt, auto, node distance=3cm
                       , semithick
                       , scale=0.5
                       ]

      \draw [-] (-10,0) rectangle (-7,-5);
      \draw [-] (-10,-1) -- (-7,-1)
                (-10,-2) -- (-7,-2)
                (-10,-3) -- (-7,-3)
                (-10,-4) -- (-7,-4);
      \draw [-] (-9,0) -- (-9,-5);
      \node () [] at (-8.5,0.5) {shared memory};
      \node () [anchor=west] at (-10,-2.5)  {\texttt{\color{blue}x}};
      \node () [anchor=west] at (-9,-2.5) {\texttt{\color{blue}0}};

      \node () [anchor=west] at (-10,-3.5)  {\texttt{\color{blue}y}};
      \node () [anchor=west] at (-9,-3.5)  {\texttt{\color{blue}0}};

      \node () [anchor=center] at (-2.5,-3.5) {store buffer};
      \draw [-] (-4.5,-4) rectangle (-0.5,-5);
      \draw [-] (-2.5,-4) -- (-2.5,-5);

      \node () [anchor=center] at (3.5,-3.5) {store buffer};
      \draw [-] (1.5,-4) rectangle (5.5,-5);
      \draw [-] (3.5,-4) -- (3.5,-5);

      \node () [anchor=west] at (-4.5,-4.5)  {\texttt{\color{red}y}};
      \node () [anchor=west] at (-2.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor=west] at (1.5,-4.5)  {\texttt{\color{red}x}};
      \node () [anchor=west] at (3.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor = west, xshift = -1em] at (-4.5, 0.5) {thread 0};
      \draw [->] (-4.5,0) -- (-4.5,-3);
      \node () [anchor=west] at (-4, -0.5) {\texttt{\color{red}y = 1;}};
      \node () [anchor=west] at (-4, -1.5) {\texttt{\color{blue}load x; \textrightarrow 0}};
      \node () [anchor=west] at (-4, -2.5) {\texttt{\color{frombuf}load y; \textrightarrow 1}};

      \node () [anchor = west, xshift = -1em] at (1.5, 0.5) {thread 1};
      \draw [->] (1.5,0) -- (1.5,-3);
      \node () [anchor=west] at (2, -0.5) {\texttt{\color{red}x = 1;}};
      \node () [anchor=west] at (2, -1.5) {\texttt{\color{blue}load y; \textrightarrow 0}};
      \node () [anchor=west] at (2, -2.5) {\texttt{\color{frombuf}load x; \textrightarrow 1}};

  \end{tikzpicture}
  \end{center}
  \end{minipage}

  \caption{
  A demonstration of the \xtso memory model.
  The thread 0 stores 1 to variable \texttt{y} and then loads variables \texttt{x} and \texttt{y}.
  The thread 1 stores 1 to \texttt{x} and then loads \texttt{y} and \texttt{x}.
  Intuitively, we would expect it to be impossible for $a = 0$ and $b = 0$ to both be true at the end of the execution, as there is no interleaving of thread actions which would produce such a result.
  However, under \xtso, the stores are cached in the store buffers (marked \textcolor{red}{red}).
  A load consults only shared memory and the store buffer of the given thread, which means it can load data from the memory and ignore newer values from the other thread (\textcolor{blue}{blue}).
  Therefore \texttt{a} and \texttt{b} will contain old values from the memory.
  On the other hand, \texttt{c} and \texttt{d} will contain local values from the store buffers (locally read values are marked \textcolor{frombuf}{green}).
  }

  \label{fig:xtso}
\end{figure}

The operational semantics of \xtso is described by Sewell et al.
in~\cite{x86tso}.
The corresponding machine has hardware threads (or cores), each with associated
local store buffer, a shared memory subsystem, and a shared memory lock.
Store buffers are first-in-first-out caches into which store entries are saved
before they are propagated to the shared memory.
Load instructions first attempt to read from the store buffer of the given
thread, and only if they are not successful, they read from the shared memory.
Store instructions push a new entry to the local store buffer.
Entries in the store buffer are not visible to threads other then the one
owning the store buffer.
Atomic instructions include various read-modify-write instructions, e.g. atomic
arithmetic operations (which take memory address and a constant),\mnote{These
  instructions have the \texttt{lock} prefix in the assembly, for example
  \texttt{lock xadd} for atomic addition.}
or compare-and-swap instruction.\mnote{\texttt{lock cmpxchg}}
All atomic instructions use the shared memory lock so that only one such
instruction can be executed at a given time, regardless of the number of
hardware threads in the machine.
Furthermore, atomic instructions flush the store buffer of their thread before
they release the lock.
This means that effects of atomic operations are immediately visible, i.e.,
atomics are sequentially consistent on \xtso.
On top of these instructions, \xtso has a full memory barrier (\texttt{mfence})
which flushes the store buffer of the thread that executed it.\mnote{There
are two more fence instructions in the x86 instruction set, but according
to~\cite{x86tso} they are not relevant to normal program execution.}

To recover sequential consistency on x86, it is necessary to make memory stores
propagate to the main memory before subsequent loads execute.
This is most commonly done in practice by inserting memory fence after each
store.
An alternative approach would be to use atomic exchange instruction
(\texttt{lock xchg}) which can atomically swap value between a register and a
memory slot.

One of the specifics of x86 is that it can handle unaligned memory
operations.\mnote{Other architectures, for example ARM, require loaded
values to be aligned, usually so that the address is divisible by the value
size.}
While the \xtso paper does not give any specifics about handling unaligned and
mixed memory operations (e.g., writing a 64-bit value and then reading a 16-bit
value from inside it) it seems from our own experiments that such the
operations are not only fully supported, but they are also correctly
synchronized if atomic instructions are used.
This is in agreement with the aforementioned operational semantics of \xtso in
which all the atomic operations share a single global lock.

\subsection{Other Hardware Memory Models}

\TODO{… už nepřevzato}

Older works on analysis of programs under relaxed memory models usually
consider the SPARC TSO, PSO and RMO memory models~\cite{SPARC94} or the memory
model of ALPHA~\cite{mckenney2010} processors.
However, these memory models are not relevant any more, with the exception of
TSO, which is sometimes used interchangeable with the \xtso memory model, as
they are very similar (the difference is that \xtso precisely describes
behaviour of atomic compound operations of x86 processors, but some works use
TSO to stand for the memory model of x86 processors too).

Currently, apart from the \xtso memory model, mostly the memory models of
variants of POWER and ARM processors are relevant.
While these processors differ significantly in the area in which they are used
(POWER is used in high-performance servers while ARM is mostly used in mobile
devices), their memory models share some basic features.
Both of these types of processors exhibit more relaxed behaviour then \xtso,
for example it is possible to reorder a load after a store, or to reorder
stores or loads with one another (provided they are independent).
\TODO{example}
On POWER it is also not guaranteed that if one thread observes some ordering of
events, other threads will observe the same ordering.
An example of such behaviour can be seen in \autoref{fig:prelim:iriw}.

\begin{figure}[tp]
    \begin{cppcode}
        int x = 0, y = 0;
    \end{cppcode}
    \begin{multicols}{4}
        \begin{cppcode}
          void writer0()
          {
            x = 1;
          }
        \end{cppcode}
        \columnbreak
        \begin{cppcode}
          void writer1()
          {
            y = 1;
          }
        \end{cppcode}
        \columnbreak
        \begin{cppcode}
          void reader0()
          {
            int x0 = x;
            int y0 = y;
          }
        \end{cppcode}
        \columnbreak
        \begin{cppcode}
          void reader1()
          {
            int y1 = y;
            int x1 = x;
          }
        \end{cppcode}
    \end{multicols}
    \center{\emph{Is it possible that $x_0 = 1 \land y_0 = 0 \land x_1 = 0 \land y_1 = 1$?}}
    \caption{
        An independent readers of independent writers example commonly used to
        demonstrate some of the features of POWER memory models.
        We assume each of the functions is executed in a separate thread.
        Here, we are asking if it can happen that while the \cpp{reader0}
        observes the new value of $x$ and old value of $y$ the \cpp{reader1}
        will observe the old value of $x$ and new value of $y$.
        Such an observation would imply that the two modifications are visible
        to the two readers in different order.
        This behaviour can indeed be observed on POWER~\cite{TODO}, but not on
        ARM~\cite{Pulte2017}.
    }\label{fig:prelim:iriw}
\end{figure}

There are several version (or generations) of ARM and POWER processors, and
also multiple versions of memory models.
The memory model of ARMv7 is described in~\cite{Alglave2014}.
The memory model of newer ARMv8 is described in~\cite{Flur2016}, but it was
later revised and simplified in collaboration with ARM and formalized
in~\cite{Pulte2017} and in the ARMv8 architecture description.
Later in~\cite{Pulte2019} an operation model of ARMv8 concurrency equivalent
with the one presented in~\cite{Pulte2017} and optimized for program analysis
was presented.
Interestingly, \cite{Pulte2019} also presents a memory model of RISC-V
architecture which is similar to the revised ARMv8 memory model.

The basis of POWER memory model was described as an abstract machine
in~\cite{Sarkar2011} and later extended in~\cite{Sarkar2012} to cover atomic
compound operations.
A more detailed memory model of POWER is presented in~\cite{Gray2015},
including for example behaviour of mixed-size memory operations.
Axiomatic description of POWER is provided by~\cite{MadorHaim2012} and
\cite{Alglave2010_fences}.
In~\cite{Flur2017} the authors describe behaviour of POWER and ARMv8 in
presence of mixed-size memory operations.

\subsection{Memory Models of Programming Languages}

\cite{Pulte2019:29} C11

\section{??? Lock Free Programming ???}

\section{Programming Languages Used in this Thesis}

In this work, we often use example codes in figures.
We will mainly use C++ for examples of high-level code written by programmers,
and LLVM IR for example of low-level code more suitable for direct analysis.

\subsection{C++}

C++ is a high-level language well suited for wide variety of projects, from
code which directly interacts with hardware to GUI applications.
It can be high-performance and has good support for building of abstractions.
Since the C++11 version of the C++ standard, C++ also has native support for
threads and atomic variables with varying levels of atomicity guarantees.

We will now show some of the C++ basics which will be used in this thesis.
Unless explicitly stated otherwise, all C++ examples in this work use the C++20
standard~\cite{cpp20}.

\paragraph{Threads}

In C++ a thread is executed by creation of an object of type \cpp{std::thread}
(\cpp{std::} is a namespace which indicates this type belongs to the C++
standard library).
This object is created with a callable (a function, a member function, or an
object which can be called) and its arguments.
It then starts a new thread which executes the given callable.
Later, the thread can be waited-for by calling the \cpp{join} method of the
\cpp{std::thread} object.
Joining will block until the thread we are joining finishes.
An example can be seen in \autoref{fig:prelim:cppthread}.

\begin{figure}[tp]
    \begin{cppcode}
        #include <thread> // make std::thread available
                          // we will often omit includes

        void workFn(int a, int b) {
            // do some work …
        }

        // program's entry point
        int main() {
            // start two worker threads

            // thread which calls workFn(1, 6)
            std::thread worker0(workFn, 1, 6);
            // thread which calls workFn(4, 2)
            std::thread worker1(workFn, 4, 2);

            // and wait for them
            worker0.join();
            worker1.join();
        }
    \end{cppcode}
    \caption{A basic example C++ program with a main thread and two worker
    threads.}\label{fig:prelim:cppthread}
\end{figure}

\paragraph{Mutexes and Other Synchronization Primitives}

C++ comes with various high-level synchronization primitives (which are usually
based on operating system level blocking primitives -- i.e., a thread waiting
on these synchronization primitives is suspended and not using any resources).

For the purposes of this work, we will use two synchronization primitives,
\emph{mutexes} and \emph{condition variables}.
Mutexes can be used to create mutual exclusion: only one thread can lock a
given mutex at any point.
Example code with mutexes is shown in \autoref{fig:prelim:cppmtx}.

\begin{figure}[tp]
    \begin{cppcode}
        #include <mutex>
        #include <thread>

        std::mutex global_lock;
        int x = 0;
        int y = 0;

        void thread0() {
            global_lock.lock();
            ++x;
            global_lock.unlock()
            ++y;
        }

        void thread1() {
            ++y; // global_lock not locked
            std::unique_lock guard(global_lock); // lock it
            // global_lock locked
            ++x;
        } // unlocks here

        int main() {
            std::thread t0(thread0);
            std::thread t1(thread1);
            t0.join(); t1.join();
            assert(x == 2); // holds, all accesses to x are properly synchronized
            assert(y == 2); // does not hold
        }
    \end{cppcode}
    \caption{An example of C++ mutexes which shows two ways to lock a mutex in
    C++.
    It is possible to either explicitly lock mutex as in function
    \cpp{thread0}, or to use scope-based resource control as in
    \cpp{thread1}.
    It is recommended to use the scope-based approach.
    With the scope-based approach, the mutex is locked when an object of type
    \cpp{std::unique_lock} is created (with given mutex as an argument),
    and it is unlocked at the end of life of the \cpp{std::unique_lock} object.
    \cpp{std::unique_lock} should be used as a local variable, therefore its
    life ends at the end of the block in which it was declared (end of function
    \cpp{thread1} in this case).
    }\label{fig:prelim:cppmtx}
\end{figure}

Condition variables allow some threads to wait for signal from other threads.
They are often used in producer-consumer scenarios to signal to consumers that
there is some data ready for them.
Condition variables are always used together with mutexes.
They have two main operations: \texttt{wait} and \texttt{signal}: \texttt{wait}
is blocking call which suspend its calling thread until another thread calls
\texttt{signal}.
Waiting can only be ended if \texttt{signal} is called after the wait started,
the conditional variable has no way to detect that \texttt{signal} was called
before \texttt{wait}.
Furthermore, due to limitations of certain platforms, \texttt{wait} is allowed
to end spuriously (without being signalled).
For these two reasons, condition variables are usually used together with a
shared variable which indicates whether or not the consumer thread should
proceed; this variable should be guarded by the same mutex as the condition
variable used for signalling.
To work around this limitation, and 
See \autoref{fig:prelim:cppcond} for an example code with condition variables.

\begin{figure}[tp]
    \begin{cppcode}
        #include <thread>
        #include <mutex>
        #include <condition_variable>

        std::mutex lock;
        std::condition_variable sig;
        bool work_available = false;

        void producer() {
            // ... save work to shared variable
            {
                std::unique_lock guard(lock);
                work_available = true;
            } // end of block -> unlock 'lock'
            sig.noify_all()
        }

        void consumer() {
            {
                // scope based locking is needed here as wait takes 'unique_lock' as one of its arguments
                std::unique_lock guard(lock);
                // the second argument of wait is a lambda function which gives condition which must hold before 'wait' can return -- if 'sig' is signalled before (or wait ends spuriously), waiting is resumed
                // please note that the lock is release while waiting and re-acquired before 'wait' returns
                sig.wait(guard, []{ return work_available; });
            } // end of block -> unlock 'lock'
            // ... now do some work with the data
        }
    \end{cppcode}
    \caption{This is a simple commented example which demonstrates usage of
    condition variables for thread signalling in producer-consumer scenario.
    }\label{fig:prelim:cppcond}
\end{figure}

\paragraph{Atomic Variables}

Apart from the high-level synchronization primitives (mutexes, condition
variables), C++ has also support for atomic variables and atomic operations
with them.
These atomic variables allow the program to take advantage of atomic hardware
instructions available on most platforms.
Atomic variables are mostly used in lock-free data structures and algorithms.

Atomic variables in C++ are variables of one of the atomic types:
\cpp{std::atomic_flag} and \cpp{std::atomic<T>} for some type \cpp{T} (which is
usually an integral of pointer type, for example \cpp{std::atomic<int>}).
For integral types, \cpp{std::atomic<T>} defines various operations which allow
atomic modification of the value of an atomic variable: for example, by calling
\cpp{fetch_add} on an atomic variable it is possible to atomically increment
its value and return its original value before the increment.
Some of these operations are also available with the operator syntax (e.g.,
using the operator \cpp{+=}).
For all types, it is possible to exchange the current value with a new one
(returning the previous value) and to preform compare-exchange (also sometimes
called compare-and-swap), which is a compound operation which atomically checks
that the value of an atomic variable is equal to a specified value and if it is
replaces the original value with a new one.

An example of use of atomic variables can be seen in
\autoref{fig:prelim:cppatomic}.

\begin{figure}[tp]
    \begin{cppcode}
        #include <atomic>
        #include <thread>

        std::atomic< int > w { 0 };
        std::atomic< int > x { 0 };
        std::atomic< int > y { 0 }
        int z = 0;

        void worker() {
            ++x; // OK, this is an atomic increment
            y.fetch_add(1); // OK, atomic increment, returns original value
            ++z; // ERROR, this is a data race
            int expected = 0;

            // OK, atomic, only one thread can succeed
            if ( w.compare_exchange_strong(expected, 1) ) {
                // 'w' changed from 0 to 1
            } else {
                // 'w' not changed
            }
        }

        int main() {
            std::thread t0(worker);
            std::thread t1(worker);
            t1.join();
            t2.join();
            assert( x == 2 ); // OK
            assert( y == 2 ); // OK
            assert( z == 2 ); // ERROR
        }
    \end{cppcode}
    \caption{This code demonstrates basic usage of atomic variables in C++.
    It shows two versions of atomic increment (with variables \texttt{x} and
    \texttt{y}) and it also demonstrates use of the \texttt{compare\_exchange}
    operation.
    }\label{fig:prelim:cppatomic}
\end{figure}

\subparagraph{Memory Ordering for Atomic Operations} \label{sec:prelim:cppmemord}

Without any additional settings all atomic instructions in C++ are sequentially
consistent i.e., there is a single global ordering of atomic operations on
which all threads agree.
However, C++ aims to allow programmers to fully utilize the performance of a
given platform and therefore it is possible to specify weaker constraints for
atomic operations.
These constraints are specified using so-called \emph{memory order}.
We will now shortly describe available memory orders (however, not describe
them precisely, as it is beyond scope and need of this work).

\begin{description}
    \item[\texttt{std::memory\_order::seq\_cst}] is the strongest and default
        memory order.
        It forces operations with this memory order to be sequentially
        consistent.

        \item[\texttt{std::memory\_order::release}] is memory order used with
            store operations (operations which write to the memory).
            It prevents other memory operations to be reordered after the
            release store.

        \item[\texttt{std::memory\_order::acquire}] is used with load operations
            (operations which read from the memory).
            It prevents other memory operations to be reordered before the
            acquire load.
            A release store and a subsequent acquire load from the same variable
            create a synchronization which ensures that all modification
            perfrormed in the storing thread before the release store are
            visible to the loading store after the corresponding acquire load.

        \item[\texttt{std::memory\_order::acq\_rel}] is used with atomic
            compound operations and combines the acquire and release orderings.

        \item[\texttt{std::memory\_order::consume}] is a weaker form of
            \texttt{acquire} which only affects data-dependent variables.
            It memory ordering is not widely used.

        \item[\texttt{std::memory\_order::relaxed}] is the weakest atomic
            ordering.
            It guarantees no synchronization and does not affect operations
            concerning other memory locations at all.
            It only guarantees atomicity of the given operations.
            Relaxed ordering can be used for example to implement atomic
            counters used for statistics.
\end{description}

\paragraph{Exceptions}

C++ has support for exceptions and is also able to specify that a given function
is not allowed to throw an exception.
In C++ it is possible to throw value of any type as an exception.
When an exception is thrown, it propagates to callers of the function which
have throw it until it triggers a \cpp{catch} code block which can catch it --
a \cpp{catch} block which either catches exactly the type of the exception, or
if the class of the exception uses inheritance, the exception can be also
caught a by a \cpp{catch} block which catches some of the predecessor types of
the exception.

If a function is marked as \cpp{noexcept} and an exception would propagate from
it, the program terminates.

An example code with exceptions can be found in \autoref{fig:prelim:cppexcept}.

\begin{figure}[tp]
    \begin{cppcode}
        // invalid input inherits from the standard exception type 'std::runtime_error'
        struct invalid_input : std::runtime_error {
            // inherit constructors
            using std::runtime_error::runtime_error;
        }

        void foo(int val) {
            if (val < 0)
                throw invalid_input("val is less then 0");
        }

        void bar() {
            try {
                foo(-5);
                // this will not execute due to exception
            } catch (invalid_input &ex) {
                // exception handler
            }
        }

        void baz() noexcept { // this function declares it does not throw an exception
            bar(); // OK, exception thrown and caught inside
            foo(42); // also OK, exception not thrown
            try {
                foo(-1); // OK, exception caught here
            } catch (std::runtime_error &ex) {
                // also catches the exception
            }
            foo(-2); // ERROR, would propagate exception out of 'bar' -> program is terminated
        }

        int main() { baz(); }
    \end{cppcode}
    \caption{A demonstration of exceptions in C++.
    We can see that an exception can be caught by a \cpp{catch} handler of its
    predecessor type (in \cpp{baz}).
    We also see that throwing an exception from a function marked as
    \cpp{noexcept} causes the program to be terminated.
    }\label{fig:prelim:cppexcept}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLVM} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LLVM is a compilation infrastructure which can be used to build optimizing
compilers.
The compiler build on LLVM consists of a language-specific frontend (which
processes the source code and produces a language independent LLVM intermediate
representation), an optimizer which runs on the LLVM intermediate
representation, and a code generator which produces assembly for the given
platform.
LLVM intermediate representation (LLVM IR, LLVM code, or just LLVM), is a
low-level programming language which is mostly independent of both the
high-level language of the original program and the assembly language of the
given hardware platform.
Nevertheless, LLVM IR is somewhat influenced by the languages that are mainly
translated to it, namely C and C++.

\subsubsection{Introduction to LLVM IR}

LLVM IR is a type-safe assembly-like language.
Its basic operations are \emph{instructions} which take inputs of specific type
and produce output of (possibly different) type.
Values can be stored either in registers (each register is only assigned at one
place in the code -- the code is static single assignment) or in memory.
Memory can be further divided into global variables, which exist for the entire
run of the program, and dynamically allocated memory, which is obtained by a
call to an allocation function provided by the platform (i.e., the allocation
function is expected to be externally provided, memory allocation is not a part
of LLVM).

\paragraph{Memory Manipulation in LLVM}
Unlike for example the x86 machine code, most LLVM instruction do not modify
memory directly, but work with registers only.
Therefore, to change a value in memory, it is first necessary to load it, using
the \li{load} instruction, then modify it, and finally store it using the
\li{store} instruction.
There are two more instructions which can access memory, and these are used for
\emph{atomic compound operations}.
The \li{atomicrmw} instruction (atomic read-modify-write) can atomically
perform a load, arithmetic or logic operation, and a store or atomically
replace a value in the memory with another value, and in all cases it returns
the old value of the memory.
The \li{cmpxchg} (compare exchange) can atomically check if the value in memory
is the same as expected, and if so, replace it with a new value.
The atomic compound operations are often used to implement \emph{lock-free}
algorithms and data structures.
Atomic compound operations have memory order argument which specifies their
level of atomicity.
Similarly, \li{load} and \li{store} instructions can be atomic and then they
also come with a memory order argument.
Memory orders in LLVM are based on memory orders in C++ (see
\autoref{sec:prelim:cppmemord}).

\paragraph{Threads in LLVM}

LLVM has no primitives for starting and handling threads.
This matches well with the programming languages often translated to LLVM,
which usually implement threading using a library of thread-manipulation and
synchronization primitives.
Nevertheless, LLVM has a notion of thread-local variables; i.e., variables
which exist in a separate copy in each thread.

\paragraph{Memory Model}

Memory model of LLVM is mostly based on the memory model of C++, but instead of
atomic variables, it uses only atomic operations (i.e., it is theoretically
possible to combine atomic and non-atomic access to the same variable in LLVM).

\paragraph{Exceptions}

LLVM has support for exceptions.
Namely, there are two ways in which a function can be called in LLVM.
The \li{call} instruction is used for calls which either cannot throw an
exception, or which can throw an exception, but the exception does not need to
be inspected or caught in the functions which performs the \li{call}.
On the other hand, the \li{invoke} instruction is used if the exception needs
to be intercepted -- an \li{invoke} is a branching point.
If the function called by \li{invoke} returns normally the \cpp{invoke} behaves
like a \li{call}.
However, if an exception is propagated through \li{invoke} it transfers control
to a block of code which starts with a \li{landingpad} instruction which can
recover information about the exception.
The code which starts with the \li{landingpad} then decides how to handle the
exception -- it can be handled by this function, or a cleanup can be run and
the propagation of the exception can be resumed using the \li{resume}
instruction.

Interestingly, there is no instruction in LLVM to throw an exception.
Instead, the whole design of exception handling in C++ assumes the platform for
which the code is compiled provides an exception support library -- so called
\emph{unwinder} which has also ability to throw the exception.
When the program is executing, the unwinder also takes care of the actual
propagation of the exception through the call stack (\emph{unwinding}).
For this purpose it needs metadata which is generated by the code generator
from the information in the \li{invoke}, \li{landingpad}, and \li{resume}
instructions.

\paragraph{Syntax of LLVM IR}

\TODO{...}

\section{DIVINE}

\TODO{MOVE: 
States $v_1, v_2$ are connected by an edge in the state space if $v_2$ can be reached from $v_1$ in an atomic step, which is a sequence of instructions that executes at most one action which can interfere with any action executed in parallel with it.

In \divine, the state space generator attempts to make the longest possible atomic step while ensuring that the generation of the edge terminates.
Edges are labelled, and the labels can be used to indicate accepting edges and error edges.
Error edges are edges on which safety violation occurs (e.g., an assertion violation or memory error).
The notion of accepting edges was taken initially from transition-based Büchi automata and used for LTL model checking, but in general, it is a way to mark an edge as interesting for the verification algorithm, but not erroneous.
These labels are set by the verified program, which can be instrumented to influence edge labelling or by \divine when it detects an error.

The state space of a program can be an infinite graph.
However, in \divine, we are primarily concerned with programs which have finite state space.
If the state space is infinite, \divine might find an error if it is present
there, or it might compute until its resources are exhausted.
Please note that programs with finite state space can have infinite behaviour as they can loop through the same set of states indefinitely.
}

\subsection{Basic Program Features}

When it comes to the program analysis of programs written in high-level programming languages (such as C, C++, C\#, Java, Haskell),\mnote{Some readers might find it peculiar to consider C to be a high-level programming language; however, for our purposes the defining characteristic of a high-level language is that it is intended to be written by the programmer and allows them to use and define abstractions -- at the very least functions and data structures. In this way C is a high-level programming language, while assembly languages (such as x86-64 assembly) are low-level.} it is often necessary to choose a level on which the program will be analysed.
We can choose to analyse the high-level code directly -- in this case the analysis can be both precise and make use of high-level concepts (such as cycles or declarative description of certain thereading concepts e.g., using Open MP\mnote{\TODO{Open MP}}).
However, the direct analysis comes at high cost as programming languages often have rich and complex syntax and complicated semantics which is designed to aid programming, not verification.
For example, if we choose to handle in this way programs written in C++, we would need to concern ourselves with object hierarchies, exception handling, templates and many more.
This approach is also specific for the given programming language.


The on the other side of the spectrum, we can analyse a compiled program i.e., analyse machine code or assembly of given platform.
This approach can then be applied regardless of programming language and it works even on programs for which we have no source code available.
Nevertheless, there are serious disadvantages -- the analysis is required to support each hardware platform separately, and the machine code looses a lot of information present on the higher level.
For example, it is not possible to recover the bounds of stack variables from the x86 machine code directly (it might be recoverable using debugging information or exception-handling metadata, but these might not be available).

Finally, we can analyse some sort of intermediate or specialized representation, either a low-level language which still presents the information necessary for the analysis, or a specialized language designed for verification.
In either case, it is first necessary to translate the code in the programming language to the language used for verification and this can share many problems with the direct approach of verification of the high-level programming language.
However, if an existing intermediate representation accompanied with a translator from the high-level programming language is suitable for verification, this can side-step many of the problems described here.
Another advantage of using an intermediate representation is that the same representation can be used for multiple programming languages and multiple platforms -- this can make adaptation to new languages or platforms easier.
See \autoref{fig:prel:verif-levels}.

\begin{figure}
\center
\begin{tikzpicture}[->, >=stealth']

    \tikzstyle{box}=[rectangle, thick, draw]

    \begin{scope}
      \matrix[nodes=box, column sep = 0.5cm]{
        \node (c) {C}; &
        \node (cpp) {C++}; &
        \node (rust) {Rust}; &
        \node (hs) {Haskell}; &
        \node (cs) {C\#}; &
        \node (fs) {F\#}; &
        \node (java) {Java}; &
        \node (scala) {Scala}; \\
      };
    \end{scope}

    \begin{scope}[yshift=-2cm]
      \matrix[nodes={box, minimum width = 2cm}, column sep = 0.5cm]{
        \node (llvm) {LLVM IR}; &
        \node (net) {???}; &
        \node (jvm) {???}; \\
      };
    \end{scope}

    \begin{scope}[yshift=-4cm]
      \matrix[nodes={box, minimum width = 2cm}, column sep = 0.5cm]{
        \node (x86) {x86}; &
        \node (x86-64) {x86-64}; &
        \node (arm7) {ARMv7}; &
        \node (arm8) {ARMv8}; \\
      };
    \end{scope}

    \draw (c.south) edge (llvm)
          (cpp.south) edge (llvm)
          (rust.south) edge (llvm)
          (hs.south) edge (llvm);
    \draw (cs.south) edge (net)
          (fs.south) edge (net);
    \draw (java.south) edge (jvm)
          (scala.south) edge (jvm);

    \draw (llvm) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);

    \draw (net) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);

    \draw (jvm) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);
%        edge [pre, draw=red] node[yshift=-1mm] {\texttt{0: lock(m)}} (i);

\end{tikzpicture}
\caption{}\label{fig:prel:verif-levels}
\end{figure}

\section{SAT and SMT}

… need these shortly
+ bitvectors

% vim: colorcolumn=80 expandtab sw=4 ts=4 spell spelllang=en
