\section{Basic Features of Realistic Programs}

When it comes to the program analysis of programs written in high-level programming languages (such as C++, C\#, Java, Haskell), it is often necessary to choose a level on which the program will be analysed.
We can choose to analyse the high-level code directly -- in this case the analysis can be both precise and make use of high-level concepts (such as cycles or declarative description of certain thereading concepts e.g., using Open MP\footnotemark{}).
However, the direct analysis comes at high cost as programming languages often have rich and complex syntax and complicated semantics which is designed to aid programming, not verification.
For example, if we choose to handle in this way programs written in C++, we would need to concern ourselves with object hierarchies, exception handling, templates and many more.
\footnotetext{\TODO{Open MP}}

The on the other side of the spectrum, we can analyse a compiled program i.e., analyse machine code or assembly of given platform.
This approach can then be applied regardless of programming language and it works even on programs for which we have no source code available.
Nevertheless, there are serious disadvantages -- the analysis is required to support each hardware platform separately, and the machine code looses a lot of information present on the higher level.
For example, it is not possible to recover the bounds of stack variables from the x86 machine code directly (it might be recoverable using debugging information or exception-handling metadata, but these might not be available).

Finally, we can analyse some sort of intermediate or specialized representation, either a low-level language which still presents the information necessary for the analysis, or a specialized language designed for verification.
In either case, it is first necessary to translate the code in the programming language to the language used for verification and this can share many problems with the direct approach of verification of the high-level programming language.
However, if an existing intermediate representation accompanied with a translator from the high-level programming language is suitable for verification, this can side-step many of the problems described here.

\subsection{Minimal Requirements for Programs}

To make our considerations mostly independent of the choice of the verified formalism, we will now define minimal requirements for such a formalism.

\begin{description}
    \item[Function Calls and Recursion]
    \item[Indirect Function Calls]
    \item[Threads] The verified formalism should have ability to spawn threads dynamically and to wait for end of a thread.
        Therefore, the number of threads which will run at any point in the program might not be known before the program is executed.
    \item[Shared Memory] There should be memory locations accessible by any thread. This memory can be used to share data between threads.
    \item[Synchronization Primitives] Primitives which allow safe synchronization of data between threads (such as mutexes and condition variables) should be available.
    \item[Atomic Operations] It has to be possible to read and modify designated memory locations with atomic operations which are safe to be executed in parallel (e.g., atomic increment, atomic compare and swap).
    \item[Pointers and Pointer Arithmetic]
\end{description}

\section{Basic Verification Concepts}

\subsection{State Space}

- finite-state programs, finite-state threads

The \emph{state space} of a program is a directed multigraph with labelled edges.
The vertices of the state space multigraph are called \emph{states} (of the program).
Each state represents a snapshot of the program (its memory, program counters and stacks of all its threads, …).
% However, for our purposes, we will consider states to be opaque and will only require the ability to determine the successors of a state from it.
States $v_1, v_2$ are connected by an edge in the state space if $v_2$ can be reached from $v_1$ in an atomic step, which is a sequence of instructions that executes at most one action which can interfere with any action executed in parallel with it.
In \divine, the state space generator attempts to make the longest possible atomic step while ensuring that the generation of the edge terminates.
Edges are labelled, and the labels can be used to indicate accepting edges and error edges.
Error edges are edges on which safety violation occurs (e.g., an assertion violation or memory error).
The notion of accepting edges was taken initially from transition-based Büchi automata and used for LTL model checking, but in general, it is a way to mark an edge as interesting for the verification algorithm, but not erroneous.
These labels are set by the verified program, which can be instrumented to influence edge labelling or by \divine when it detects an error.

The state space of a program can be an infinite graph.
However, in \divine, we are primarily concerned with programs which have finite state space.
If the state space is infinite, \divine might find an error if it is present
there, or it might compute until its resources are exhausted.
Please note that programs with finite state space can have infinite behaviour as they can loop through the same set of states indefinitely.

\section{Program Properties}

\subsection{Safety Properties}

example: leak checking -> is it safety or LTL?

\subsection{Temporal Logics}

CTL* -> LTL

\subsection{Fairness}

\subsection{Termination Properties}

\section{Realistic Programs}

\subsection{High-Level Programming Languages}

\subsection{Libraries \& Environment}

\subsection{Hardware-Related Considerations}

- word size, required alignment, memory model

\section{Parallelism \& Threading Model}

\subsection{Relaxed Memory Models}

The relaxed behavior of processors arises from optimizations in cache consistency protocols and observable effects of instructions reordering and speculation.
The effect of this behavior is that memory-manipulating instructions can appear to be executed in a different order than the order in which they appear in the binary, and their effect can even appear to be in different order on different threads.
For efficiency reasons, virtually all modern processors (except for very simple ones in microcontrollers) exhibit relaxed behavior.
The extent of this relaxation is dependent on the processor architecture (e.g., x86, ARM, POWER) but also on the concrete processor model.
Furthermore, the actual behavior of the processor is often not precisely described by the processor vendor \cite{x86tso}.
To abstract from the details of particular processor models, \emph{relaxed memory models} are used to describe (often formally) behavior of processor architectures.
Examples of relaxed memory models of modern processors are the memory model of x86 and x86\_64 CPUs described formally as \xtso~\cite{x86tso} and the multiple variants of POWER~\cite{Sarkar2011,Mador-Haim2012} and ARM~\cite{Flur2016,Alglave2014,Pulte2017} memory models.

For the description of a memory model, it is sufficient to consider operations which affect the memory.
These operations include loads (reading of data from the memory to a register in the processor), stores (writing of data from a register to the memory), memory barriers (which constrain memory relaxation), and atomic compound operations (read-modify-write operations and compare-and-swap operation).

\subsection{The \xtso Memory Model}

The \xtso is very similar to the SPARC Total Store Order (TSO) memory model~\cite{SPARC94}.
It does not reorder stores with each other, and it also does not reorder loads with other loads.
The only relaxation allowed by \xtso is that store can appear to be executed later than a load which succeeds it.
The memory model does not give any limit on how long a store can be delayed.
An example of non-intuitive execution of a simple program under \xtso can be found in Figure~\ref{fig:xtso}.

\begin{figure}[th] % fig:xtso
    \begin{minipage}[b]{0.25\textwidth}
    \tt
    \textcolor{gray}{int} x = 0, y = 0;
    \par\smallskip
    \textcolor{gray}{void} thread0() \{ \\
    \indent{}y = 1; \\
    \indent{}\textcolor{gray}{int} a = x; \\
    \indent{}\textcolor{gray}{int} c = y; \\
    \}
    \par\smallskip
    \textcolor{gray}{void} thread1() \{ \\
    \indent{}x = 1; \\
    \indent{}\textcolor{gray}{int} b = y; \\
    \indent{}\textcolor{gray}{int} d = x; \\
    \}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}[b]{0.73\textwidth}
    \begin{center}
    \noindent
    Is $a = 0 \land b = 0$ reachable?\\[2.5ex]
    \begin{tikzpicture}[ ->, >=stealth', shorten >=1pt, auto, node distance=3cm
                       , semithick
                       , scale=0.5
                       ]

      \draw [-] (-10,0) rectangle (-7,-5);
      \draw [-] (-10,-1) -- (-7,-1)
                (-10,-2) -- (-7,-2)
                (-10,-3) -- (-7,-3)
                (-10,-4) -- (-7,-4);
      \draw [-] (-9,0) -- (-9,-5);
      \node () [] at (-8.5,0.5) {shared memory};
      \node () [anchor=west] at (-10,-2.5)  {\texttt{\color{blue}x}};
      \node () [anchor=west] at (-9,-2.5) {\texttt{\color{blue}0}};

      \node () [anchor=west] at (-10,-3.5)  {\texttt{\color{blue}y}};
      \node () [anchor=west] at (-9,-3.5)  {\texttt{\color{blue}0}};

      \node () [anchor=center] at (-2.5,-3.5) {store buffer};
      \draw [-] (-4.5,-4) rectangle (-0.5,-5);
      \draw [-] (-2.5,-4) -- (-2.5,-5);

      \node () [anchor=center] at (3.5,-3.5) {store buffer};
      \draw [-] (1.5,-4) rectangle (5.5,-5);
      \draw [-] (3.5,-4) -- (3.5,-5);

      \node () [anchor=west] at (-4.5,-4.5)  {\texttt{\color{red}y}};
      \node () [anchor=west] at (-2.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor=west] at (1.5,-4.5)  {\texttt{\color{red}x}};
      \node () [anchor=west] at (3.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor = west, xshift = -1em] at (-4.5, 0.5) {thread 0};
      \draw [->] (-4.5,0) -- (-4.5,-3);
      \node () [anchor=west] at (-4, -0.5) {\texttt{\color{red}y = 1;}};
      \node () [anchor=west] at (-4, -1.5) {\texttt{\color{blue}load x; \textrightarrow 0}};
      \node () [anchor=west] at (-4, -2.5) {\texttt{\color{frombuf}load y; \textrightarrow 1}};

      \node () [anchor = west, xshift = -1em] at (1.5, 0.5) {thread 1};
      \draw [->] (1.5,0) -- (1.5,-3);
      \node () [anchor=west] at (2, -0.5) {\texttt{\color{red}x = 1;}};
      \node () [anchor=west] at (2, -1.5) {\texttt{\color{blue}load y; \textrightarrow 0}};
      \node () [anchor=west] at (2, -2.5) {\texttt{\color{frombuf}load x; \textrightarrow 1}};

  \end{tikzpicture}
  \end{center}
  \end{minipage}

  \caption{
  A demonstration of the \xtso memory model.
  The thread 0 stores 1 to variable \texttt{y} and then loads variables \texttt{x} and \texttt{y}.
  The thread 1 stores 1 to \texttt{x} and then loads \texttt{y} and \texttt{x}.
  Intuitively, we would expect it to be impossible for $a = 0$ and $b = 0$ to both be true at the end of the execution, as there is no interleaving of thread actions which would produce such a result.
  However, under \xtso, the stores are cached in the store buffers (marked \textcolor{red}{red}).
  A load consults only shared memory and the store buffer of the given thread, which means it can load data from the memory and ignore newer values from the other thread (\textcolor{blue}{blue}).
  Therefore \texttt{a} and \texttt{b} will contain old values from the memory.
  On the other hand, \texttt{c} and \texttt{d} will contain local values from the store buffers (locally read values are marked \textcolor{frombuf}{green}).
  }

  \label{fig:xtso}
\end{figure}

The operational semantics of \xtso is described by Sewell et al. in~\cite{x86tso}.
The corresponding machine has hardware threads (or cores), each with associated local store buffer, a shared memory subsystem, and a shared memory lock.
Store buffers are first-in-first-out caches into which store entries are saved before they are propagated to the shared memory.
Load instructions first attempt to read from the store buffer of the given
thread, and only if they are not succesfull, they read from the shared memory.
Store instructions push a new entry to the local store buffer.
Atomic instructions include various read-modify-write instructions, e.g. atomic
arithmetic operations (which take memory address and a constant),\footnote{These
  instructions have the \texttt{lock} prefix in the assembly, for example
  \texttt{lock xadd} for atomic addition.}
or compare-and-swap instruction.\footnote{\texttt{lock cmpxchg}}
All atomic instructions use the shared memory lock so that only one such instruction can be executed at a given time, regardless of the number of hardware threads in the machine.
Furthermore, atomic instructions flush the store buffer of their thread before they release the lock.
This means that effects of atomic operations are immediately visible, i.e., atomics are sequentially consistent on \xtso.
On top of these instructions, \xtso has a full memory barrier (\texttt{mfence}) which flushes the store buffer of the thread that executed it.\footnote{There are two more fence instructions in the x86 instruction set, but according to~\cite{x86tso} they are not relevant to normal program execution.}

To recover sequential consistency on x86, it is necessary to make memory stores propagate to the main memory before subsequent loads execute.
This is most commonly done in practice by inserting memory fence after each store.
An alternative approach would be to use atomic exchange instruction
(\texttt{lock xchg}) which can atomically swap value between a register and a
memory slot.

One of the specifics of x86 is that it can handle unaligned memory operations.\footnote{Other architectures, for example ARM, require loaded values to be aligned, usually so that the address is divisible by the value size.}
While the \xtso paper does not give any specifics about handling unaligned and
mixed memory operations (e.g., writing a 64-bit value and then reading a 16-bit
value from inside it) it seems from our own experiments that such the operations
are not only fully supported, but they are also correctly synchronized if atomic instructions are used.
This is in agreement with the aforementioned operational semantics of \xtso in
which all the atomic operations share a single global lock.


\section{Verification Techniqes}

\subsection{Explicit-State Model Checking}

\subsection{Bounded Model Checking}

\subsection{Stateless Model Checking}

\section{LLVM \& LLVM Intermediate Representation}

LLVM is a compilation infrastructure which can be used to build optimizing compilers.
The entire compiler build on LLVM consists of a language-specific frontend (which processes the source code and produces a language independent LLVM intermediate representation), an optimizer build which runs on the LLVM intermediate representation, and a code generator which produces assembly for the given platform.
LLVM intermediate representation (LLVM IR, LLVM code, or just LLVM), is a low-level programming language which is both mostly independent of the high-level language of the original program and the assembly language of the given hardware platform.
\TODO{...}

- memory manipulation
- atomic instructions, memory model
- thereads
- exceptions

\section{C++}

The examples of code in high-level programming languages in this thesis will be given in C++.
Therefore we will now shortly discuss some basic concepts of this programming language which are necessary for understanding of the examples.

- atomic variables
- mutexes
- threads
- scoped variables \& RAII
