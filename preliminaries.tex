In this chapter we define problems we will be focusing on, and give elementary goals for the analysis tool -- what it needs to be able to handle to be considered useful.
We will also present concepts which we will use later in the work and introduce programming languages used in the examples.

\section{Realistic Programs}

In order for a program analysis tool to be useful to programmers, it should be able to process the code they are creating with a minimal extra effort.
For this reason, program analysis tools should be able to process some of the mainstream high-level programming languages preferably without limitations to the features of these languages which can be understood by the program analyser.
For example a tools which understands basics of the C language, but does not allow dynamic memory allocation, is not very useful for a programmer, as they will probably not be able to  use it to analyse their usual programs which contain dynamic memory allocation.

On the other hand, if the language support is good enough, it enables the programmer to use the analysis tool directly on their program or its fragments, both during the development, and for older code as the programmer is not forced to simplify, rewrite or model their code in order to get usable analysis results.

\subsection{High-Level Programming Languages}

\subsection{Libraries \& Environment}

\subsection{Hardware-Related Considerations}

- word size, required alignment, memory model

\section{Basic Features of Programs}

When it comes to the program analysis of programs written in high-level programming languages (such as C, C++, C\#, Java, Haskell),\footnotemark{} it is often necessary to choose a level on which the program will be analysed.
We can choose to analyse the high-level code directly -- in this case the analysis can be both precise and make use of high-level concepts (such as cycles or declarative description of certain thereading concepts e.g., using Open MP\footnotemark{}).
However, the direct analysis comes at high cost as programming languages often have rich and complex syntax and complicated semantics which is designed to aid programming, not verification.
For example, if we choose to handle in this way programs written in C++, we would need to concern ourselves with object hierarchies, exception handling, templates and many more.
This approach is also specific for the given programming language.

\footnotetext{Some readers might find it peculiar to consider C to be a high-level programming language; however, for our purposes the defining characteristic of a high-level language is that it is intended to be written by the programmer and allows them to use and define abstractions -- at the very least functions and data structures. In this way C is a high-level programming language, while assembly languages (such as x86-64 assembly) are low-level.}
\footnotetext{\TODO{Open MP}}

The on the other side of the spectrum, we can analyse a compiled program i.e., analyse machine code or assembly of given platform.
This approach can then be applied regardless of programming language and it works even on programs for which we have no source code available.
Nevertheless, there are serious disadvantages -- the analysis is required to support each hardware platform separately, and the machine code looses a lot of information present on the higher level.
For example, it is not possible to recover the bounds of stack variables from the x86 machine code directly (it might be recoverable using debugging information or exception-handling metadata, but these might not be available).

Finally, we can analyse some sort of intermediate or specialized representation, either a low-level language which still presents the information necessary for the analysis, or a specialized language designed for verification.
In either case, it is first necessary to translate the code in the programming language to the language used for verification and this can share many problems with the direct approach of verification of the high-level programming language.
However, if an existing intermediate representation accompanied with a translator from the high-level programming language is suitable for verification, this can side-step many of the problems described here.
Another advantage of using an intermediate representation is that the same representation can be used for multiple programming languages and multiple platforms -- this can make adaptation to new languages or platforms easier.
See \autoref{fig:prel:verif-levels}.

\begin{figure}
\center
\begin{tikzpicture}[->, >=stealth']

    \tikzstyle{box}=[rectangle, thick, draw]

    \begin{scope}
      \matrix[nodes=box, column sep = 0.5cm]{
        \node (c) {C}; &
        \node (cpp) {C++}; &
        \node (rust) {Rust}; &
        \node (hs) {Haskell}; &
        \node (cs) {C\#}; &
        \node (fs) {F\#}; &
        \node (java) {Java}; &
        \node (scala) {Scala}; \\
      };
    \end{scope}

    \begin{scope}[yshift=-2cm]
      \matrix[nodes={box, minimum width = 2cm}, column sep = 0.5cm]{
        \node (llvm) {LLVM IR}; &
        \node (net) {???}; &
        \node (jvm) {???}; \\
      };
    \end{scope}

    \begin{scope}[yshift=-4cm]
      \matrix[nodes={box, minimum width = 2cm}, column sep = 0.5cm]{
        \node (x86) {x86}; &
        \node (x86-64) {x86-64}; &
        \node (arm7) {ARMv7}; &
        \node (arm8) {ARMv8}; \\
      };
    \end{scope}

    \draw (c.south) edge (llvm)
          (cpp.south) edge (llvm)
          (rust.south) edge (llvm)
          (hs.south) edge (llvm);
    \draw (cs.south) edge (net)
          (fs.south) edge (net);
    \draw (java.south) edge (jvm)
          (scala.south) edge (jvm);

    \draw (llvm) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);

    \draw (net) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);

    \draw (jvm) edge (x86)
                 edge (x86-64)
                 edge (arm7)
                 edge (arm8);
%        edge [pre, draw=red] node[yshift=-1mm] {\texttt{0: lock(m)}} (i);

\end{tikzpicture}
\caption{}\label{fig:prel:verif-levels}
\end{figure}

\subsection{Minimal Requirements for Programs}

To make our considerations mostly independent of the choice of the verified formalism, we will now define minimal requirements for such a formalism.

\begin{description}
    \item[Function Calls and Recursion]
    \item[Indirect Function Calls]
    \item[Threads] The verified formalism should have ability to spawn threads dynamically and to wait for end of a thread.
        Therefore, the number of threads which will run at any point in the program might not be known before the program is executed.
    \item[Shared Memory] There should be memory locations accessible by any thread. This memory can be used to share data between threads.
    \item[Synchronization Primitives] Primitives which allow safe synchronization of data between threads (such as mutexes and condition variables) should be available.
    \item[Atomic Operations] It has to be possible to read and modify designated memory locations with atomic operations which are safe to be executed in parallel (e.g., atomic increment, atomic compare and swap).
    \item[Pointers and Pointer Arithmetic]
    \item[Assertions]
\end{description}

\subsection{Additional Requirements for Programs}

Sometimes we will also consider additional features which many programming languages share, but which we do not consider to be critical in a programming language used for concurrent software.

\begin{description}
    \item[Exception Handling]
    \item[Run-Time Type Support \& Dynamic Dispatch]
\end{description}

\section{Basic Verification Concepts}

\subsection{State Space}

- finite-state programs, finite-state threads
- run

The \emph{state space} of a program is a directed multigraph with labelled edges.
The vertices of the state space multigraph are called \emph{states} (of the program).
Each state represents a snapshot of the program (its memory, program counters and stacks of all its threads, …).
% However, for our purposes, we will consider states to be opaque and will only require the ability to determine the successors of a state from it.
States $v_1, v_2$ are connected by an edge in the state space if $v_2$ can be reached from $v_1$ in an atomic step, which is a sequence of instructions that executes at most one action which can interfere with any action executed in parallel with it.
In \divine, the state space generator attempts to make the longest possible atomic step while ensuring that the generation of the edge terminates.
Edges are labelled, and the labels can be used to indicate accepting edges and error edges.
Error edges are edges on which safety violation occurs (e.g., an assertion violation or memory error).
The notion of accepting edges was taken initially from transition-based Büchi automata and used for LTL model checking, but in general, it is a way to mark an edge as interesting for the verification algorithm, but not erroneous.
These labels are set by the verified program, which can be instrumented to influence edge labelling or by \divine when it detects an error.

The state space of a program can be an infinite graph.
However, in \divine, we are primarily concerned with programs which have finite state space.
If the state space is infinite, \divine might find an error if it is present
there, or it might compute until its resources are exhausted.
Please note that programs with finite state space can have infinite behaviour as they can loop through the same set of states indefinitely.

\section{Program Properties}

In order to be able to find errors in programs an analysis tool has to have some specification of program correctness -- a \emph{property} which should be checked.
In this section, we will introduce several categories of program properties together with examples of particular properties.

\subsection{Safety Properties}

\emph{Safety properties} are properties which can be checked locally -- for each state of the program, we are able to determine if the state satisfies or violates given property.
Therefore, to conclude the program is error-free it suffices to show that there is no state which violates given safety property.

Examples of safety property include \emph{memory safety} (stating that every time we access a certain part of memory, this memory is in fact allocated and accessible to the program, see also \autoref{fig:pre:memsafe}), \emph{assertion safety} (stating that a program does not violate any of the assertions in the source code, see also \autoref{fig:pre:assert}), and \emph{control-flow definedness} (stating that every time a conditional jump is performed, the values the jump is based on must have well-defined value, see also \autoref{fig:pre:definedness}).

example: leak checking -> is it safety or LTL?

\subsection{Temporal Properties}

Not all properties can be directly described as safety properties.
For example, we can have a property stating ,,every time a button is pressed, the elevator will eventually drive to the floor the button was pressed on'' -- such a property cannot be checked directly in one state of the program, but instead needs to be checked on a \emph{run} of the program.

For specification of temporal properties, we will use temporal logic CTL*~\cite{todo}, and its linear fragment, LTL (Linear Time Logic)~\cite{TODO}.

\begin{definition}[CTL*]

\end{definition}

\begin{definition}[LTL]
\end{definition}

- more expressive temporal logics…

\subsection{Fairness}

\subsection{Termination Properties}

A distinguished form of temporal properties are termination properties, which are properties which allow us to specify that a program must (or must not) terminate, or that some of its part must (or must not) terminate.
An example of a termination property concerning a part of the program can be found in \autoref{fig:pre:term}.

\subsection{Other Properties}

\section{Parallelism \& Threading Model}

\subsection{Relaxed Memory Models}

The relaxed behavior of processors arises from optimizations in cache consistency protocols and observable effects of instructions reordering and speculation.
The effect of this behavior is that memory-manipulating instructions can appear to be executed in a different order than the order in which they appear in the binary, and their effect can even appear to be in different order on different threads.
For efficiency reasons, virtually all modern processors (except for very simple ones in microcontrollers) exhibit relaxed behavior.
The extent of this relaxation is dependent on the processor architecture (e.g., x86, ARM, POWER) but also on the concrete processor model.
Furthermore, the actual behavior of the processor is often not precisely described by the processor vendor \cite{x86tso}.
To abstract from the details of particular processor models, \emph{relaxed memory models} are used to describe (often formally) behavior of processor architectures.
Examples of relaxed memory models of modern processors are the memory model of x86 and x86\_64 CPUs described formally as \xtso~\cite{x86tso} and the multiple variants of POWER~\cite{Sarkar2011,Mador-Haim2012} and ARM~\cite{Flur2016,Alglave2014,Pulte2017} memory models.

For the description of a memory model, it is sufficient to consider operations which affect the memory.
These operations include loads (reading of data from the memory to a register in the processor), stores (writing of data from a register to the memory), memory barriers (which constrain memory relaxation), and atomic compound operations (read-modify-write operations and compare-and-swap operation).

\subsection{The \xtso Memory Model}

The \xtso is very similar to the SPARC Total Store Order (TSO) memory model~\cite{SPARC94}.
It does not reorder stores with each other, and it also does not reorder loads with other loads.
The only relaxation allowed by \xtso is that store can appear to be executed later than a load which succeeds it.
The memory model does not give any limit on how long a store can be delayed.
An example of non-intuitive execution of a simple program under \xtso can be found in Figure~\ref{fig:xtso}.

\begin{figure}[th] % fig:xtso
    \begin{minipage}[b]{0.25\textwidth}
    \tt
    \textcolor{gray}{int} x = 0, y = 0;
    \par\smallskip
    \textcolor{gray}{void} thread0() \{ \\
    \indent{}y = 1; \\
    \indent{}\textcolor{gray}{int} a = x; \\
    \indent{}\textcolor{gray}{int} c = y; \\
    \}
    \par\smallskip
    \textcolor{gray}{void} thread1() \{ \\
    \indent{}x = 1; \\
    \indent{}\textcolor{gray}{int} b = y; \\
    \indent{}\textcolor{gray}{int} d = x; \\
    \}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}[b]{0.73\textwidth}
    \begin{center}
    \noindent
    Is $a = 0 \land b = 0$ reachable?\\[2.5ex]
    \begin{tikzpicture}[ ->, >=stealth', shorten >=1pt, auto, node distance=3cm
                       , semithick
                       , scale=0.5
                       ]

      \draw [-] (-10,0) rectangle (-7,-5);
      \draw [-] (-10,-1) -- (-7,-1)
                (-10,-2) -- (-7,-2)
                (-10,-3) -- (-7,-3)
                (-10,-4) -- (-7,-4);
      \draw [-] (-9,0) -- (-9,-5);
      \node () [] at (-8.5,0.5) {shared memory};
      \node () [anchor=west] at (-10,-2.5)  {\texttt{\color{blue}x}};
      \node () [anchor=west] at (-9,-2.5) {\texttt{\color{blue}0}};

      \node () [anchor=west] at (-10,-3.5)  {\texttt{\color{blue}y}};
      \node () [anchor=west] at (-9,-3.5)  {\texttt{\color{blue}0}};

      \node () [anchor=center] at (-2.5,-3.5) {store buffer};
      \draw [-] (-4.5,-4) rectangle (-0.5,-5);
      \draw [-] (-2.5,-4) -- (-2.5,-5);

      \node () [anchor=center] at (3.5,-3.5) {store buffer};
      \draw [-] (1.5,-4) rectangle (5.5,-5);
      \draw [-] (3.5,-4) -- (3.5,-5);

      \node () [anchor=west] at (-4.5,-4.5)  {\texttt{\color{red}y}};
      \node () [anchor=west] at (-2.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor=west] at (1.5,-4.5)  {\texttt{\color{red}x}};
      \node () [anchor=west] at (3.5,-4.5)  {\texttt{\color{red}1}};

      \node () [anchor = west, xshift = -1em] at (-4.5, 0.5) {thread 0};
      \draw [->] (-4.5,0) -- (-4.5,-3);
      \node () [anchor=west] at (-4, -0.5) {\texttt{\color{red}y = 1;}};
      \node () [anchor=west] at (-4, -1.5) {\texttt{\color{blue}load x; \textrightarrow 0}};
      \node () [anchor=west] at (-4, -2.5) {\texttt{\color{frombuf}load y; \textrightarrow 1}};

      \node () [anchor = west, xshift = -1em] at (1.5, 0.5) {thread 1};
      \draw [->] (1.5,0) -- (1.5,-3);
      \node () [anchor=west] at (2, -0.5) {\texttt{\color{red}x = 1;}};
      \node () [anchor=west] at (2, -1.5) {\texttt{\color{blue}load y; \textrightarrow 0}};
      \node () [anchor=west] at (2, -2.5) {\texttt{\color{frombuf}load x; \textrightarrow 1}};

  \end{tikzpicture}
  \end{center}
  \end{minipage}

  \caption{
  A demonstration of the \xtso memory model.
  The thread 0 stores 1 to variable \texttt{y} and then loads variables \texttt{x} and \texttt{y}.
  The thread 1 stores 1 to \texttt{x} and then loads \texttt{y} and \texttt{x}.
  Intuitively, we would expect it to be impossible for $a = 0$ and $b = 0$ to both be true at the end of the execution, as there is no interleaving of thread actions which would produce such a result.
  However, under \xtso, the stores are cached in the store buffers (marked \textcolor{red}{red}).
  A load consults only shared memory and the store buffer of the given thread, which means it can load data from the memory and ignore newer values from the other thread (\textcolor{blue}{blue}).
  Therefore \texttt{a} and \texttt{b} will contain old values from the memory.
  On the other hand, \texttt{c} and \texttt{d} will contain local values from the store buffers (locally read values are marked \textcolor{frombuf}{green}).
  }

  \label{fig:xtso}
\end{figure}

The operational semantics of \xtso is described by Sewell et al. in~\cite{x86tso}.
The corresponding machine has hardware threads (or cores), each with associated local store buffer, a shared memory subsystem, and a shared memory lock.
Store buffers are first-in-first-out caches into which store entries are saved before they are propagated to the shared memory.
Load instructions first attempt to read from the store buffer of the given
thread, and only if they are not succesfull, they read from the shared memory.
Store instructions push a new entry to the local store buffer.
Atomic instructions include various read-modify-write instructions, e.g. atomic
arithmetic operations (which take memory address and a constant),\footnote{These
  instructions have the \texttt{lock} prefix in the assembly, for example
  \texttt{lock xadd} for atomic addition.}
or compare-and-swap instruction.\footnote{\texttt{lock cmpxchg}}
All atomic instructions use the shared memory lock so that only one such instruction can be executed at a given time, regardless of the number of hardware threads in the machine.
Furthermore, atomic instructions flush the store buffer of their thread before they release the lock.
This means that effects of atomic operations are immediately visible, i.e., atomics are sequentially consistent on \xtso.
On top of these instructions, \xtso has a full memory barrier (\texttt{mfence}) which flushes the store buffer of the thread that executed it.\footnote{There are two more fence instructions in the x86 instruction set, but according to~\cite{x86tso} they are not relevant to normal program execution.}

To recover sequential consistency on x86, it is necessary to make memory stores propagate to the main memory before subsequent loads execute.
This is most commonly done in practice by inserting memory fence after each store.
An alternative approach would be to use atomic exchange instruction
(\texttt{lock xchg}) which can atomically swap value between a register and a
memory slot.

One of the specifics of x86 is that it can handle unaligned memory operations.\footnote{Other architectures, for example ARM, require loaded values to be aligned, usually so that the address is divisible by the value size.}
While the \xtso paper does not give any specifics about handling unaligned and
mixed memory operations (e.g., writing a 64-bit value and then reading a 16-bit
value from inside it) it seems from our own experiments that such the operations
are not only fully supported, but they are also correctly synchronized if atomic instructions are used.
This is in agreement with the aforementioned operational semantics of \xtso in
which all the atomic operations share a single global lock.


\section{Verification Techniqes}

\subsection{Explicit-State Model Checking}

\subsection{Bounded Model Checking}

\subsection{Stateless Model Checking}

\section{LLVM \& LLVM Intermediate Representation}

LLVM is a compilation infrastructure which can be used to build optimizing compilers.
The compiler build on LLVM consists of a language-specific frontend (which processes the source code and produces a language independent LLVM intermediate representation), an optimizer which runs on the LLVM intermediate representation, and a code generator which produces assembly for the given platform.
LLVM intermediate representation (LLVM IR, LLVM code, or just LLVM), is a low-level programming language which is both mostly independent of the high-level language of the original program and the assembly language of the given hardware platform, which makes it a good basis for program analysis.

\subsection{Introduction to LLVM IR}

LLVM IR is a type-safe assembly-like language.
Its basic operations are \emph{instructions} which take inputs of specific type and produce output of (possibly different) type.
Values can be stored either in registers (each register is only assigned at one place in the code -- the code is static single assignment) or in memory.
Memory can be further divided into global variables, which exist for the entire run of the program, and dynamically allocated memory, which is obtained by a call to an allocation function provided by the platform (i.e., the allocation function is expected to be externally provided, memory allocation is not a part of LLVM).

\paragraph{Memory Manipulation in LLVM}
Unlike for example the x86 machine code, most LLVM instruction do not modify memory directly, but work with registers only.
Therefore, to change a value in memory, it is first necessary to load it, using the \li{load} instruction, then modify it, and finally store it using the \li{store} instruction.
There are two more instructions which can access memory, and these are used for \emph{atomic compound operations}.
The \li{atomicrmw} instruction (atomic read-modify-write) can atomically perform a load, arithmetic or logic operation, and a store or atomically replace a value in the memory with another value, and in all cases it returns the old value of the memory.
The \li{cmpxchg} (compare exchange) can atomically check if the value in memory is the same as expected, and if so, replace it with a new value.
The atomic compound operations are often used to implement \emph{lock-free} algorithms and data structures.

\paragraph{Threads in LLVM}

LLVM has no primitives for starting and handling threads.
This matches well with the programming languages often translated to LLVM, which usually implement threading using a library of thread-manipulation and synchronization primitives.
Nevertheless, LLVM has a notion of thread-local variables; i.e., variables which exist in a separate copy in each thread.

\paragraph{Memory Model}



\paragraph{Exceptions}

\paragraph{Syntax of LLVM IR}

In some of the examples in this work, we will use the human-readable version of LLVM IR to present a low-level code.
Here, we will demonstrate basics of LLVM IR syntax on an example.

\llvminput[lastline=4]{llexample.ll}

\noindent
At its beginning an LLVM module contains information about the source from which it was created and about the platform for which it is compiled (these information include sizes of various data types).

\llvminput[firstline=6, lastline=6]{llexample.ll}

\noindent
Named types can be declared in LLVM, their name always starts with the \texttt{\%} sigil.
The content of the type is given in the braces, in this case it is a data structure containing a 32 bit integer, a 64 bit integer, and a float.
This code snipped can be obtained by compiling the following C/C++ declaration on a 64 bit Linux system: \cpp{struct Foo { int x; long y; float z; }}.

\llvminput[firstline=8, lastline=8]{llexample.ll}

\noindent
Global variables (as well as functions) have names starting with the \texttt{@} sigil.
Global variables are defined with their type (\li{i32} or a 32 bit integer in this case), alignment and linking options (which are largely irrelevant for this work).
The type used in the declaration of a global variable is a type of its value, however, the type of the corresponding name expression (\li{@g} in this case) is a pointer to this type (\li{i32*}) as global variables has to be accessed using the memory manipulation instructions.


\llvminput[linenos, firstline=10]{llexample.ll}

\TODO{...}

\section{C++}

The examples of code in high-level programming languages in this thesis will be given in C++.
Therefore we will now shortly discuss some basic concepts of this programming language which are necessary for understanding of the examples.

- atomic variables
- mutexes
- threads
- scoped variables \& RAII
